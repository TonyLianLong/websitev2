---
title: "LLM-grounded Diffusion: Enhancing Prompt Understanding of Text-to-Image Diffusion Models with Large Language Models"
collection: publications
excerpt: 'Enhancing the prompt understanding capabilities of text-to-image diffusion models with large-language models for grounding.'
date: 2023-05-22
venue: ''
paper_url: 'https://arxiv.org/pdf/2305.13655.pdf'
video_url: 'https://llm-grounded-diffusion.github.io/'
demo_url: 'https://huggingface.co/spaces/longlian/llm-grounded-diffusion'
code_url: 'https://github.com/TonyLianLong/LLM-groundedDiffusion'
bibtex_url: 'https://llm-grounded-diffusion.github.io/#citation'
authors: '**Long Lian**, Boyi Li, Adam Yala, Trevor Darrell'
citation:
cover_image: 2023-05-22-LMD.gif
---
Recent advancements in text-to-image generation with diffusion models have yielded remarkable results synthesizing highly realistic and diverse images. However, these models still encounter difficulties when generating images from prompts that demand spatial or common sense reasoning. We propose to equip diffusion models with enhanced reasoning capabilities by using off-the-shelf pretrained large language models (LLMs) in a novel two-stage generation process. First, we adapt an LLM to be a text-guided layout generator through in-context learning. When provided with an image prompt, an LLM outputs a scene layout in the form of bounding boxes along with corresponding individual descriptions. Second, we steer a diffusion model with a novel controller to generate images conditioned on the layout. Both stages utilize frozen pretrained models without any LLM or diffusion model parameter optimization. We validate the superiority of our design by demonstrating its ability to outperform the base diffusion model in accurately generating images according to prompts that necessitate both language and spatial reasoning. Additionally, our method naturally allows dialog-based scene specification and is able to handle prompts in a language that is not well-supported by the underlying diffusion model.
