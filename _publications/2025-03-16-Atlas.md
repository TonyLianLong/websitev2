---
title: "Atlas: Multi-Scale Attention Improves Long Context Image Modeling"
collection: publications
excerpt: 'Atlas is a new neural network using Multi-Scale Attention for efficient cross-scale image modeling. Atlas achieves state-of-the-art accuracy with significantly better speed and compute efficiency on high-resolution image tasks.'
date: 2025-03-16
paper_url: 'https://arxiv.org/abs/2503.12355'
code_url: 'https://github.com/yalalab/atlas'
bibtex_url: 'https://github.com/yalalab/atlas#citation'
authors: '**Kumar Krishna Agrawal\***, **Long Lian\***, Longchao Liu, Natalia Harguindeguy, Boyi Li, Alexander Bick, Maggie Chung, Trevor Darrell, Adam Yala'
citation:
cover_image: 2025-03-16-Atlas.png
---
Efficiently modeling massive images is a long-standing challenge in machine learning. To this end, we introduce Multi-Scale Attention (MSA). MSA relies on two key ideas, (i) multi-scale representations (ii) bi-directional cross-scale communication. MSA creates O(log N) scales to represent the image across progressively coarser features and leverages cross-attention to propagate information across scales. We then introduce Atlas, a novel neural network architecture based on MSA. We demonstrate that Atlas significantly improves the compute-performance tradeoff of long-context image modeling in a high-resolution variant of ImageNet 100. At 1024px resolution, Atlas-B achieves 91.04% accuracy, comparable to ConvNext-B (91.92%) while being 4.3x faster. Atlas is 2.95x faster and 7.38% better than FasterViT, 2.25x faster and 4.96% better than LongViT. In comparisons against MambaVision-S, we find Atlas-S achieves 5%, 16% and 32% higher accuracy at 1024px, 2048px and 4096px respectively, while obtaining similar runtimes.
