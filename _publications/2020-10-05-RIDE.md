---
title: "Long-tailed Recognition by Routing Diverse Distribution-aware Experts"
collection: publications
excerpt: 'Leveraging bias-variance tradeoff, we proposed a universal classification framework for long-tailed distributed data with RoutIng Diverse Experts (RIDE) that improves both accuracy and inference speed.'
date: 2020-10-05
venue: 'ICLR 2021 (**Spotlight**)'
paper_url: 'https://arxiv.org/pdf/2010.01809'
video_url: 'https://iclr.cc/virtual/2021/spotlight/3456'
authors: 'Xudong Wang, **Long Lian**, Zhongqi Miao, Ziwei Liu, Stella X Yu'
citation:
cover_image: 2020-10-05-RIDE.png
---
Natural data are often long-tail distributed over semantic classes. Existing recognition methods tend to focus on gaining performance on tail classes, often at the expense of losing performance on head classes and with increased classifier variance. The low tail performance manifests itself in large inter-class confusion and high classifier variance. We aim to reduce both the bias and the variance of a long-tailed classifier by RoutIng Diverse Experts (RIDE), consisting of three components: 1) a shared architecture for multiple classifiers (experts); 2) a distribution-aware diversity loss that encourages more diverse decisions for classes with fewer training instances; and 3) an expert routing module that dynamically assigns more ambiguous instances to additional experts. With on-par computational complexity, RIDE significantly outperforms the state-of-the-art methods by 5% to 7% on all the benchmarks including CIFAR100-LT, ImageNet-LT and iNaturalist 2018. RIDE is also a universal framework that can be applied to different backbone networks and integrated into various long-tailed algorithms and training mechanisms for consistent performance gains.
