---
title: "Unsupervised Visual Attention and Invariance for Reinforcement Learning"
collection: publications
permalink: /publication/VAI
excerpt: 'Rather than training a universal RL policy invariant to train-test distribution shift, we proposed unsupervised visual attention and invariance method (VAI) to disperse interference task-irrelevant factors towards a RL policy robust to distractions.'
date: 2021-04-07
venue: '*IEEE/CVF Conference on Computer Vision and Pattern Recognition* (CVPR), 2021'
paper_url: 'https://arxiv.org/abs/2104.02921'
code_url: 'https://github.com/TonyLianLong/VAI-ReinforcementLearning'
bibtex_url: 'https://github.com/TonyLianLong/VAI-ReinforcementLearning#citation'
poster: vai_poster.png
authors: 'Xudong Wang*, **Long Lian\***, Stella X. Yu'
citation:
cover_image: 2021-04-07-VAI-animation.gif
---
The vision-based reinforcement learning (RL) has achieved tremendous success. However, generalizing vision-based RL policy to unknown test environments still remains as a challenging problem. Unlike previous works that focus on training a universal RL policy that is invariant to discrepancies between test and training environment, we focus on developing an independent module to disperse interference factors irrelevant to the task, thereby providing"" clean"" observations for the RL policy. The proposed unsupervised visual attention and invariance method (VAI) contains three key components: 1) an unsupervised keypoint detection model which captures semantically meaningful keypoints in observations; 2) an unsupervised visual attention module which automatically generates the distraction-invariant attention mask for each observation; 3) a self-supervised adapter for visual distraction invariance which reconstructs distraction-invariant attention mask from observations with artificial disturbances generated by a series of foreground and background augmentations. All components are optimized in an unsupervised way, without manual annotation or access to environment internals, and only the adapter is used during inference time to provide distraction-free observations to RL policy. VAI empirically shows powerful generalization capabilities and significantly outperforms current state-of-the-art (SOTA) method by 15% to 49% in DeepMind Control suite benchmark and 61% to 229% in our proposed robot manipulation benchmark, in term of cumulative rewards per episode.
